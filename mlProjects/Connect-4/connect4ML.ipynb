{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2d9106-90e2-41b1-b0cf-65458b54e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daanyal/Documents/GitHub/MyML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir = \"/home/daanyal/Documents/GitHub/MyML/\"\n",
    "os.chdir(dir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57475c27-21ab-497d-acff-82f915bc3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daanyal/Documents/GitHub/MyML/mlProjects/Connect-4\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"mlProjects/Connect-4/\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "649f95ea-eef4-43c7-9852-f8f03b0b0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "import pydot\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from game import game as ConnectGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0000c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Interface: Any agents for this game must inherit from this type\n",
    "    \"\"\"\n",
    "    def __init__(self, env:ConnectGame, plr:int):\n",
    "        self.env = env\n",
    "        self.plr = plr\n",
    "        self.learning = True\n",
    "        self.stateConverter = Agent.StateConverter()\n",
    "        self.defaultPlr = plr # plr should be trained as the default plr, else it will help the other player win\n",
    "    \n",
    "    def switchPlayer(self, plr:int):\n",
    "        self.plr = plr\n",
    "    \n",
    "    def step(self):\n",
    "        move = self.getMove()\n",
    "        state, reward, done = self.env.step(move, self.plr)\n",
    "\n",
    "    def getMove(self, state=None):\n",
    "        if state:\n",
    "            state = self.stateConverter.convertState(state)\n",
    "            \n",
    "        moves = self.env.getLegalMoves()\n",
    "        return np.random.choice(moves)\n",
    "    \n",
    "    class StateConverter:\n",
    "        def convertState(self, state):\n",
    "            # Default state is np array which takes the size of the gameboard:\n",
    "            #    -1: empty,  0: player 1,   1: player 2\n",
    "            self.shape = state.shape\n",
    "            return state\n",
    "        \n",
    "        def getStateShape(self) -> int | tuple:\n",
    "            return self.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "125b8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction:\n",
    "    def evaluate_state(self, state):\n",
    "        return 0\n",
    "    def evaluate_states(self, states):\n",
    "        return [0 for s in states]\n",
    "    def update_state(self, state, qVal):\n",
    "        return\n",
    "    def update_states(self, states, qVals):\n",
    "        return\n",
    "\n",
    "class TableQFunction(QFunction):\n",
    "    def __init__(self, stateShape):\n",
    "        rows, cols = stateShape[0], stateShape[1]\n",
    "        possibleCols = 2**(rows+1) - 1 # Number of different possible columns that can exist\n",
    "        self.qTable = np.ones(possibleCols ** cols) * 110\n",
    "    \n",
    "    def evaluate_state(self, state):\n",
    "        return self.qTable[state]\n",
    "    def evaluate_states(self, states):\n",
    "        return np.array([self.qTable[state] for state in states])\n",
    "    def update_state(self, state, qVal):\n",
    "        assert type(qVal) == float or type(qVal) == np.float64, f\"Invalid qVal: {qVal}\"\n",
    "        self.qTable[state] = qVal\n",
    "    def update_states(self, states, qVals):\n",
    "        assert len(states) == len(qVals), \"Unequal number of states and Q-values\" \n",
    "        for i in range(len(states)):\n",
    "            self.update_state(states[i], qVals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5257abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetQFunction(QFunction):\n",
    "    def __init__(self, stateShape:tuple, architecture:list[int]=[200,200,1]):\n",
    "        self.inputSize = stateShape[0]*stateShape[1]\n",
    "        self.architecture = architecture\n",
    "        # use tpu here if available:\n",
    "        self.initNetwork()\n",
    "    \n",
    "    def initNetwork(self): # reminder: implement transfer learning\n",
    "        self.model = keras.Sequential([\n",
    "            layers.Dense(self.architecture[0], activation=\"relu\", input_shape=(self.inputSize,))\n",
    "        ])\n",
    "        for layer in self.architecture[1:]:\n",
    "            self.model.add(layer, activation=\"relu\")\n",
    "        \n",
    "        \n",
    "        self.model.compile()\n",
    "\n",
    "    def evaluate_state(self, state):\n",
    "        pass\n",
    "    def evaluate_states(self, states):\n",
    "        pass\n",
    "    def update_state(self, state, qVal):\n",
    "        pass\n",
    "    def update_states(self, states, qVals):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTester:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38dd499",
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory = namedtuple(\"Memory\", (\"state\",\"q\", \"next_state\", \"next_q\", \"reward\"))\n",
    "\n",
    "class GameMemory:\n",
    "    def __init__(self):\n",
    "        self.memories = []\n",
    "    \n",
    "    def addMemory(self, mem:Memory):\n",
    "        self.memories.append(mem)\n",
    "    \n",
    "    def popMemory(self) -> Memory:\n",
    "        return self.memories.pop()\n",
    "    \n",
    "    def hasMemory(self) -> bool:\n",
    "        return len(self.memories) > 0\n",
    "    \n",
    "    def countMemories(self) -> int:\n",
    "        return len(self.memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a35bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledParameter:\n",
    "    def __init__(self, value:float):\n",
    "        self.value = value\n",
    "        self.valueFloor = 0\n",
    "        self.valueCeil = 1\n",
    "    \n",
    "    def get(self) -> float:\n",
    "        return self.value\n",
    "    \n",
    "    def step(self):\n",
    "        self.value = max(min(self.value, self.valueCeil), self.valueFloor)\n",
    "    def reset(self):\n",
    "        return\n",
    "    def set(self, value:float):\n",
    "        self.value = value\n",
    "class ExponentialDecayParameter(ScheduledParameter):\n",
    "    def __init__(self, initialValue:float, decayRate:float, valueFloor:float = 0,valueCeil=1):\n",
    "        ScheduledParameter.__init__(self,initialValue)\n",
    "        self.initialValue = initialValue\n",
    "        self.decayRate = decayRate\n",
    "        self.valueFloor = valueFloor\n",
    "    def step(self):\n",
    "        self.value *= self.decayRate\n",
    "        return ScheduledParameter.step(self)\n",
    "    def reset(self):\n",
    "        self.value = self.initialValue\n",
    "class LinearDecayParameter(ExponentialDecayParameter):\n",
    "    def step(self):\n",
    "        self.value -= self.decayRate\n",
    "        return ScheduledParameter.step(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64154d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterConfiguration:\n",
    "    def __init__(self, epsilon=ExponentialDecayParameter(1,0.995,0.1),\n",
    "                 gamma=ScheduledParameter(0.7), stepSize=ScheduledParameter(0.3)):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.stepSize = stepSize\n",
    "    def resetAll(self):\n",
    "        self.epsilon.reset()\n",
    "        self.gamma.reset()\n",
    "        self.stepSize.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b57ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugVariableStore:\n",
    "    def __init__(self):\n",
    "        self.nextStateVals = None\n",
    "        self.actions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f06aceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, env:ConnectGame, plr:int, configuration:ParameterConfiguration = ParameterConfiguration()):\n",
    "        Agent.__init__(self, env, plr)\n",
    "        self.stateConverter = QLearningAgent.EncodingStateConverter()\n",
    "        self.qFunction = TableQFunction(env.gameboard.shape)\n",
    "\n",
    "        self.debugVariables = DebugVariableStore()\n",
    "\n",
    "        self.switchPlayer(plr)\n",
    "\n",
    "        self.parameterConfig = configuration\n",
    "        self.resetHyperParams()\n",
    "\n",
    "        self.currentGameMemory = GameMemory()\n",
    "\n",
    "    def switchPlayer(self, plrNum:int):\n",
    "        Agent.switchPlayer(self, plrNum)\n",
    "        self.plr = plrNum\n",
    "        self.stateSelectors = (np.amax, np.argmax)\n",
    "        if plrNum != self.defaultPlr:\n",
    "            self.stateSelectors = (np.amin, np.argmin)\n",
    "\n",
    "    def resetHyperParams(self):\n",
    "        self.epsilon = self.parameterConfig.epsilon\n",
    "        self.gamma = self.parameterConfig.gamma\n",
    "        self.stepSize = self.parameterConfig.stepSize\n",
    "        self.parameterConfig.resetAll()\n",
    "\n",
    "    def episodeEnded(self): #learning and stuff\n",
    "        memories = self.currentGameMemory\n",
    "        i = 0\n",
    "        nextStateQ = 0\n",
    "        while memories.hasMemory():\n",
    "            lastMem = memories.popMemory()\n",
    "            state, q, next_state, next_q, reward = lastMem\n",
    "\n",
    "            for state, qVal, reward in ((next_state, next_q, reward), (state, q, 0)):\n",
    "                targetQ = reward + self.gamma.get() * nextStateQ\n",
    "                assert type(nextStateQ) == float or type(nextStateQ) == int, f\"nextStateQ val type should be a float, not {nextStateQ}\"\n",
    "                assert type(targetQ) == float or type(targetQ) == int, \"invalid target Q value\"\n",
    "\n",
    "                qVal += self.stepSize.get() * (targetQ - qVal)#Q + stepSize(targetQ - Q)\n",
    "                \n",
    "                assert type(qVal) == np.float64, f\"Invalide Q val: {qVal}\"\n",
    "                #trainData[i] = [state, qVal]\n",
    "                self.qFunction.update_state(state, qVal)\n",
    "\n",
    "                nextStateQ = targetQ # Or set it to updated Q? \n",
    "                i+=1\n",
    "        \n",
    "        self.currentGameMemory = GameMemory()\n",
    "        self.epsilon.step()\n",
    "        \n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        self.currentMemory = [] #namedTuple\n",
    "\n",
    "        move = self.getMove()\n",
    "\n",
    "        nextState, reward, done = self.env.step(move, self.plr)\n",
    "\n",
    "        encodedNext = self.stateConverter.convertState(nextState)\n",
    "\n",
    "        if done:\n",
    "            self.currentMemory[2] = encodedNext\n",
    "        elif not self.currentMemory[2] == encodedNext:\n",
    "            print(self.currentMemory[2])\n",
    "            print(encodedNext)\n",
    "            print(nextState)\n",
    "        assert self.currentMemory[2] == encodedNext, \"Bug spotted: Bug likely exists in getMove\"\n",
    "\n",
    "        self.currentMemory.append(reward)\n",
    "        x = self.currentMemory\n",
    "\n",
    "        self.currentGameMemory.addMemory(Memory(x[0],x[1],x[2],x[3],x[4]))\n",
    "\n",
    "        if done:\n",
    "            self.episodeEnded()\n",
    "        \n",
    "    def getMove(self): ## need to store:\n",
    "        ## state, Q, next_state, nextQ, reward, numActions, epsilon\n",
    "        state = self.env.getState(self.plr)\n",
    "        state = self.stateConverter.convertState(state)\n",
    "\n",
    "        self.currentMemory.append(state)\n",
    "\n",
    "        actions = self.env.getLegalMoves()\n",
    "\n",
    "        self.debugVariables.actions = self.env.getLegalMoves()\n",
    "\n",
    "        if actions[0] == -1:\n",
    "            stateVal = self.qFunction.evaluate_state(state)\n",
    "            self.currentMemory.append(stateVal)\n",
    "            self.currentMemory.append(state)\n",
    "            self.currentMemory.append(stateVal)\n",
    "            return -1\n",
    "        \n",
    "        nextStates = self.env.peekActions(actions, self.plr)\n",
    "        nextStates = [self.stateConverter.convertState(state) for state in nextStates]\n",
    "        nextStates.append(state)\n",
    "        stateVals = self.qFunction.evaluate_states(nextStates)\n",
    "\n",
    "        self.debugVariables.nextStateVals = stateVals.copy()\n",
    "\n",
    "        if self.epsilon.get() < np.random.random():\n",
    "            act = self.stateSelectors[1](stateVals[:-1])\n",
    "            topStateVal, topAction = self.stateSelectors[0](stateVals[:-1]), actions[act]\n",
    "        else:\n",
    "            act = np.random.randint(0, len(actions))\n",
    "            topAction = actions[act]\n",
    "            topStateVal=stateVals[act]##dsfsdfdsfcsdvfdds\n",
    "\n",
    "        self.currentMemory.append(stateVals[-1])\n",
    "        self.currentMemory.append(nextStates[act])\n",
    "        self.currentMemory.append(topStateVal)\n",
    "        return topAction\n",
    "        \n",
    "\n",
    "    class Configuration:\n",
    "        def __init__(self):\n",
    "            self.epsilon = 1 ## start, stop, schedule\n",
    "    \n",
    "    class EncodingStateConverter(Agent.StateConverter):\n",
    "        def __init__(self):\n",
    "            self.shape = (1)\n",
    "        def convertState(self, state):\n",
    "            state = state.copy()\n",
    "            state+=1\n",
    "            rows, cols = state.shape[0], state.shape[1]\n",
    "\n",
    "            multiplyTable = np.array([ [2**(rows-i-1) ] for i in range(rows) ])\n",
    "            state = state * multiplyTable\n",
    "            colVal = 2**(rows+1) - 1\n",
    "            state = state * np.array([(colVal)**(i) for i in range(cols)])\n",
    "            state = state.sum()\n",
    "            return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d1a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baae9521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 possible columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got all possible states\n",
      "0 / 923521\n",
      "2000 / 923521\n",
      "4000 / 923521\n",
      "6000 / 923521\n",
      "8000 / 923521\n",
      "10000 / 923521\n",
      "12000 / 923521\n",
      "14000 / 923521\n",
      "16000 / 923521\n",
      "18000 / 923521\n",
      "20000 / 923521\n",
      "22000 / 923521\n",
      "24000 / 923521\n",
      "26000 / 923521\n",
      "28000 / 923521\n",
      "30000 / 923521\n",
      "32000 / 923521\n",
      "34000 / 923521\n",
      "36000 / 923521\n",
      "38000 / 923521\n",
      "40000 / 923521\n",
      "42000 / 923521\n",
      "44000 / 923521\n",
      "46000 / 923521\n",
      "48000 / 923521\n",
      "50000 / 923521\n",
      "52000 / 923521\n",
      "54000 / 923521\n",
      "56000 / 923521\n",
      "58000 / 923521\n",
      "60000 / 923521\n",
      "62000 / 923521\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(failed \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(encodedStates))\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m unit_test()\n",
      "Cell \u001b[0;32mIn[25], line 55\u001b[0m, in \u001b[0;36munit_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m converted \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encodedStates, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnit test failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mstate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m == state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(possibleStates[x])\u001b[38;5;241m.\u001b[39mT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     encodedStates\u001b[38;5;241m.\u001b[39mappend(converted)\n\u001b[0;32m---> 55\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(failed \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(encodedStates))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Encoding state converter unit test\n",
    "def unit_test_getPossibleColumns(rows):\n",
    "    possibleCols = [[-1]*rows,[0],[1]]\n",
    "    if rows == 1:\n",
    "        return possibleCols\n",
    "    possibleUppers = unit_test_getPossibleColumns(rows-1)\n",
    "    for i in range(1,3):\n",
    "        for upper in possibleUppers:\n",
    "            res = upper + possibleCols[i]\n",
    "            res = [-1]*(rows - len(res)) + res\n",
    "            if not res in possibleCols:\n",
    "                possibleCols.append(res)\n",
    "                \n",
    "    return [possibleCols[0]] + possibleCols[3:]\n",
    "\n",
    "def unit_test():\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    converter = QLearningAgent.EncodingStateConverter()\n",
    "    possibleColumns = unit_test_getPossibleColumns(rows)\n",
    "\n",
    "    encodedStates = []\n",
    "    for c in possibleColumns:\n",
    "        state = np.array([c]).T\n",
    "        converted = converter.convertState(state)\n",
    "        assert converted not in encodedStates, f\"Unit test failed:\\nstate {converted}:\\n{state} == state {encodedStates.index(converted)}\"\n",
    "        encodedStates.append(converted)\n",
    "\n",
    "    print(f\"There are {len(possibleColumns)} possible columns\")\n",
    "    \n",
    "    possibleStates = [[c] for c in possibleColumns]\n",
    "    i = 1\n",
    "    while i < rows:\n",
    "        x = []\n",
    "        for j in range(len(possibleStates)):\n",
    "            for c in possibleColumns:\n",
    "                x.append(possibleStates[j] + [c])\n",
    "        possibleStates = x\n",
    "        i+=1\n",
    "    print(\"Got all possible states\")\n",
    "    encodedStates = []\n",
    "    failed = 0\n",
    "    i = 0\n",
    "    for state in possibleStates:\n",
    "        if i % 2000 == 0:\n",
    "            print(f\"{i} / {len(possibleStates)}\")\n",
    "        state = np.array(state).T\n",
    "        converted = converter.convertState(state)\n",
    "        x=-1\n",
    "        if converted in encodedStates:\n",
    "            failed+=1\n",
    "            x = encodedStates.index(converted)\n",
    "        assert converted not in encodedStates, f\"Unit test failed:\\nstate {converted}:\\n{state} == state {x}:\\n{np.array(possibleStates[x]).T}\"\n",
    "        encodedStates.append(converted)\n",
    "        i+=1\n",
    "    print(failed / len(encodedStates))\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f9d108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting game 0\n",
      "Epsilon: 1\n",
      "Player 1 won 0.00% of the last 400 games\n",
      "0.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 400\n",
      "Epsilon: 0.8186898039137951\n",
      "Player 1 won 41.50% of the last 400 games\n",
      "28.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 800\n",
      "Epsilon: 0.6702529950324074\n",
      "Player 1 won 42.25% of the last 400 games\n",
      "26.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 1200\n",
      "Epsilon: 0.548729293075715\n",
      "Player 1 won 43.25% of the last 400 games\n",
      "27.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 1600\n",
      "Epsilon: 0.44923907734991153\n",
      "Player 1 won 42.25% of the last 400 games\n",
      "26.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 2000\n",
      "Epsilon: 0.3677874521460121\n",
      "Player 1 won 37.50% of the last 400 games\n",
      "31.75% of the last 400 games were drawn\n",
      "\n",
      "starting game 2400\n",
      "Epsilon: 0.3011038370793723\n",
      "Player 1 won 38.75% of the last 400 games\n",
      "32.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 2800\n",
      "Epsilon: 0.24651064133620196\n",
      "Player 1 won 41.25% of the last 400 games\n",
      "27.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 3200\n",
      "Epsilon: 0.2018157486181985\n",
      "Player 1 won 45.25% of the last 400 games\n",
      "28.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 3600\n",
      "Epsilon: 0.1652244956629483\n",
      "Player 1 won 39.75% of the last 400 games\n",
      "27.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 4000\n",
      "Epsilon: 0.13526760995605422\n",
      "Player 1 won 42.00% of the last 400 games\n",
      "23.75% of the last 400 games were drawn\n",
      "\n",
      "starting game 4400\n",
      "Epsilon: 0.11074221307080971\n",
      "Player 1 won 45.25% of the last 400 games\n",
      "32.25% of the last 400 games were drawn\n",
      "\n",
      "starting game 4800\n",
      "Epsilon: 0.1\n",
      "Player 1 won 43.00% of the last 400 games\n",
      "31.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 5200\n",
      "Epsilon: 0.1\n",
      "Player 1 won 36.00% of the last 400 games\n",
      "28.75% of the last 400 games were drawn\n",
      "\n",
      "starting game 5600\n",
      "Epsilon: 0.1\n",
      "Player 1 won 43.25% of the last 400 games\n",
      "31.25% of the last 400 games were drawn\n",
      "\n",
      "starting game 6000\n",
      "Epsilon: 0.1\n",
      "Player 1 won 44.00% of the last 400 games\n",
      "23.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 6400\n",
      "Epsilon: 0.1\n",
      "Player 1 won 40.75% of the last 400 games\n",
      "26.75% of the last 400 games were drawn\n",
      "\n",
      "starting game 6800\n",
      "Epsilon: 0.1\n",
      "Player 1 won 40.25% of the last 400 games\n",
      "29.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 7200\n",
      "Epsilon: 0.1\n",
      "Player 1 won 36.75% of the last 400 games\n",
      "28.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 7600\n",
      "Epsilon: 0.1\n",
      "Player 1 won 36.00% of the last 400 games\n",
      "29.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 8000\n",
      "Epsilon: 0.1\n",
      "Player 1 won 44.50% of the last 400 games\n",
      "25.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 8400\n",
      "Epsilon: 0.1\n",
      "Player 1 won 45.00% of the last 400 games\n",
      "23.00% of the last 400 games were drawn\n",
      "\n",
      "starting game 8800\n",
      "Epsilon: 0.1\n",
      "Player 1 won 42.75% of the last 400 games\n",
      "24.50% of the last 400 games were drawn\n",
      "\n",
      "starting game 9200\n",
      "Epsilon: 0.1\n",
      "Player 1 won 39.50% of the last 400 games\n",
      "25.75% of the last 400 games were drawn\n",
      "\n",
      "starting game 9600\n",
      "Epsilon: 0.1\n",
      "Player 1 won 41.75% of the last 400 games\n",
      "25.50% of the last 400 games were drawn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numGames = 10000\n",
    "\n",
    "game = ConnectGame(4,5,4)\n",
    "#ExponentialDecayParameter(1,0.9995,0.1)\n",
    "agent0 = QLearningAgent(env=game, plr=0,\n",
    "                        configuration=ParameterConfiguration(epsilon=ExponentialDecayParameter(1,0.9995,0.1),\n",
    "                                                             stepSize=ScheduledParameter(0.2),\n",
    "                                                             gamma=ScheduledParameter(0.2)))\n",
    "agent1 = QLearningAgent(env=game, plr=1,\n",
    "                        configuration=ParameterConfiguration(epsilon=ExponentialDecayParameter(1,0.9995,0.1),\n",
    "                                                                            stepSize=ScheduledParameter(0.2),\n",
    "                                                             gamma=ScheduledParameter(0.2)))\n",
    "agents = [agent0,agent1]\n",
    "plr1Wins = 0\n",
    "draws = 0\n",
    "for gameNum in range(numGames):\n",
    "    if gameNum % 400 == 0:\n",
    "        print(f\"starting game {gameNum}\")\n",
    "        print(f\"Epsilon: {agent0.epsilon.get()}\")\n",
    "        print(f\"Player 1 won {(plr1Wins/400):.2%} of the last 400 games\")\n",
    "        print(f\"{(draws/400):.2%} of the last 400 games were drawn\\n\")\n",
    "        \n",
    "        plr1Wins = 0\n",
    "        draws = 0\n",
    "        ## Gonna try making plr 2 qFunction the same as plr 1 every x games.\n",
    "        ## Could try as either a copy, so it learns independantly, or a reference,\n",
    "        ## so it is literally playing against itself the same way a person does in chess\n",
    "        #agent1.qFunction.qTable = agent0.qFunction.qTable.copy()\n",
    "    \n",
    "    i=0\n",
    "    while not game.done:\n",
    "        agents[i%2].step()\n",
    "        i+=1\n",
    "    agents[i%2].step()\n",
    "    if game.winner == 0:\n",
    "        plr1Wins += 1\n",
    "    elif game.winner == -1:\n",
    "        draws += 1\n",
    "\n",
    "    game.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting game 0\n",
      "|---||---||---|\n",
      "|   ||   ||   |\n",
      "|---||---||---|\n",
      "|   ||   ||   |\n",
      "|---||---||---|\n",
      "|   ||   ||   |\n",
      "|---||---||---|\n",
      "|   ||   ||   |\n",
      "|---||---||---|\n",
      "|   ||   ||   |\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "\n",
      "|---||---||---|\n",
      "|   ||   ||   |\n",
      "|---||---||---|\n",
      "| x ||   ||   |\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x ||   ||   |\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x ||   ||   |\n",
      "|---||---||---|\n",
      "| o || x ||   |\n",
      "\n",
      "NO\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x ||   ||   |\n",
      "|---||---||---|\n",
      "| o || x ||   |\n",
      "\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x ||   ||   |\n",
      "|---||---||---|\n",
      "| o || x || x |\n",
      "\n",
      "NO\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x ||   ||   |\n",
      "|---||---||---|\n",
      "| o || x || x |\n",
      "\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x || x ||   |\n",
      "|---||---||---|\n",
      "| o || x || x |\n",
      "\n",
      "NO\n",
      "|---||---||---|\n",
      "| o ||   ||   |\n",
      "|---||---||---|\n",
      "| x || x ||   |\n",
      "|---||---||---|\n",
      "| o || x || x |\n",
      "\n",
      "|---||---||---|\n",
      "| o || x ||   |\n",
      "|---||---||---|\n",
      "| x || x ||   |\n",
      "|---||---||---|\n",
      "| o || x || x |\n",
      "\n",
      "Winner is player 2\n"
     ]
    }
   ],
   "source": [
    "# testing...\n",
    "numGames = 1\n",
    "oldEps = agent0.epsilon.get()\n",
    "agent0.epsilon.set(0)\n",
    "for gameNum in range(numGames):\n",
    "    if gameNum % 20 == 0:\n",
    "        print(f\"starting game {gameNum}\")\n",
    "    game.show()\n",
    "    time.sleep(1)\n",
    "    i=0\n",
    "    while not game.done:\n",
    "        agents[i%2].step()\n",
    "        game.show()\n",
    "        time.sleep(1)\n",
    "        print()\n",
    "        i+=1\n",
    "    agents[i%2].step()\n",
    "\n",
    "    if game.winner == -1:\n",
    "        print(\"Game was a draw\")\n",
    "    else:\n",
    "        print(f\"Winner is player {game.winner+1}\")\n",
    "    game.reset()\n",
    "agent0.epsilon.set(oldEps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59d8bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.QLearningAgent object at 0x7f8566a18a10>, <__main__.HumanAgent object at 0x7f85669b5b10>]\n",
      "0\n",
      "1\n",
      "starting game 0\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[-2.68670561e-06 -3.29676240e-05  1.17134783e-08 -2.67615343e-06\n",
      " -2.32805711e-06  1.29227883e-09]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o ||   ||   |\n",
      "plr 0 value: 1.171347828593813e-08\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[-2.68670561e-06 -3.29676240e-05  1.17134783e-08 -2.67615343e-06\n",
      " -2.32805711e-06  1.29227883e-09]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || x ||   |\n",
      "plr 0 value: 6.422723173880652e-08\n",
      "\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[-4.17054664e-05 -1.48434580e-06  2.99643696e-07  1.01663403e-06\n",
      " -2.46331490e-07  6.42272317e-08]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || x ||   |\n",
      "plr 0 value: 1.0166340303568619e-06\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[-4.17054664e-05 -1.48434580e-06  2.99643696e-07  1.01663403e-06\n",
      " -2.46331490e-07  6.42272317e-08]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || x ||   |\n",
      "plr 0 value: 1.2682126761304504\n",
      "\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[ 9.44909231 18.45493591 18.45493939 18.45493888 18.4549355   1.26821268]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || x ||   |\n",
      "plr 0 value: 18.45493938747772\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[ 9.44909231 18.45493591 18.45493939 18.45493888 18.4549355   1.26821268]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || x || x |\n",
      "plr 0 value: 88.00001024\n",
      "\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[70.3999488  87.99999992 88.00000205 88.0000512  87.9999488  88.00001024]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || x || x |\n",
      "plr 0 value: 88.0000512\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[70.3999488  87.99999992 88.00000205 88.0000512  87.9999488  88.00001024]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[110. 110. 110. 110. 110. 110.]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[110. 110. 110. 110. 110. 110.]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "|   ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| x ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[110. 110. 110. 110. 110. 110.]\n",
      "|---||---||---||---||---|\n",
      "|   ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "| o ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| x ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[110. 110. 110. 110. 110. 110.]\n",
      "|---||---||---||---||---|\n",
      "| x ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "| o ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| x ||   || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "plr 0's turn:\n",
      "State vals:\n",
      "[110. 110. 110. 110. 110.]\n",
      "|---||---||---||---||---|\n",
      "| x ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "| o ||   || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| x || o || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "plr 1's turn:\n",
      "State vals:\n",
      "[110. 110. 110. 110. 110.]\n",
      "|---||---||---||---||---|\n",
      "| x ||   ||   ||   ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| x || o || x || o ||   |\n",
      "|---||---||---||---||---|\n",
      "| o || x || o || x || x |\n",
      "plr 0 value: 110.0\n",
      "\n",
      "Winner is player 2\n"
     ]
    }
   ],
   "source": [
    "## Play vs human test\n",
    "# testing...\n",
    "\n",
    "class HumanAgent(Agent):\n",
    "    def step(self):\n",
    "        move = int(input(\"Where to place? (1-3)\")) - 1\n",
    "        state, reward, done = self.env.step(move, self.plr)\n",
    "\n",
    "numGames = 1\n",
    "oldEps = agent0.epsilon.get()\n",
    "agent0.epsilon.set(0)\n",
    "oldAgents = agents\n",
    "\n",
    "agents = [agent0, HumanAgent(game, 1)]\n",
    "\n",
    "#agents.reverse()\n",
    "\n",
    "agents[0].switchPlayer(0)\n",
    "agents[1].switchPlayer(1)\n",
    "print(agents)\n",
    "print(agents[0].plr)\n",
    "print(agents[1].plr)\n",
    "\n",
    "agent0.debugVariables = DebugVariableStore()\n",
    "agent1.debugVariables = DebugVariableStore()\n",
    "game.reset()\n",
    "for gameNum in range(numGames):\n",
    "    if gameNum % 20 == 0:\n",
    "        print(f\"starting game {gameNum}\")\n",
    "    game.show()\n",
    "    time.sleep(0.5)\n",
    "    i=0\n",
    "    while not game.done:\n",
    "        agents[i%2].step()\n",
    "        print(f\"plr {i%2}'s turn:\")\n",
    "        print(f\"State vals:\\n{agent0.debugVariables.nextStateVals}\")\n",
    "        game.show()\n",
    "        print(f\"plr 0 value: {agent0.qFunction.evaluate_state(agent0.stateConverter.convertState(game.getState(0)))}\")\n",
    "        #print(f\"plr 1 value: {agent0.qFunction.evaluate_state(agent1.stateConverter.convertState(game.getState(0)))}\")\n",
    "\n",
    "        ### TODO: Improve exploration by randomly choosing action from the set of equally good actions\n",
    "        #print(f\"plr 1 State vals:\\n{agent1.debugVariables.nextStateVals}\")\n",
    "        time.sleep(0.5)\n",
    "        #input(\"Press enter to continue...\")\n",
    "        print()\n",
    "        i+=1\n",
    "    agents[i%2].step()\n",
    "\n",
    "    if game.winner == -1:\n",
    "        print(\"Game was a draw\")\n",
    "    else:\n",
    "        print(f\"Winner is player {game.winner+1}\")\n",
    "    game.reset()\n",
    "agents = oldAgents\n",
    "agent0.epsilon.set(oldEps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
